{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7922e149-72fa-48cd-a545-b70bddb40d28",
    "_uuid": "9341f2516086ca38bca96e06a9dbfc39f813a95a"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# \"../input/\" 경로로 데이터 파일을 input 할 수 있습니다.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f06173b0-d354-4693-904e-a87f840ec372",
    "_uuid": "41648b358ed7c6c682ad50b85a347595733e2214"
   },
   "source": [
    "**GPU에서 실행** : CPU와 관련하여 일부 호환성 문제가 있습니다.\n",
    "1. 딥러닝에는 하이퍼파라미터가 많아서, 튜닝하는데 몇 주 또는 몇 달이 걸립니다. 일반적으로 연구자들은 튜닝을 하고, 그들이 다른 것들보다 더 나은 성능을 가진 아키텍처를 발견했을 때 논문을 발표합니다.\n",
    "\n",
    "2. 이 모델은 pre-trained 모델이기 때문에 매우 빠르게 수렴되었지만 그래도 이 모델을 사용하려면 GPU가 필요합니다. 일부 라이브러리 문제로 인해 CPU에서는 작동하지 않습니다.\n",
    "\n",
    "3. 우리의 목적을 위해, 우리는 연구자들에 의해서 이용할 수 있도록 만들어진 아키텍처들을 사용할 수 있습니다.\n",
    "\n",
    "4. 이미 피처를 추출할 방법을 알고 있는 레이어인 pre-trained nets 를 이용하면 우리는 하이퍼파라미터들을 튜닝할 필요가 없습니다. 이미 일부 데이터셋(예:imagenet)에 대해 train되었으므로, pre-trained weight는 적절한 weight의 초기화를 제공하고, 이로 인해 우리의 Convnet은 딥 아키텍처에 매우 빠르게 수렴합니다. 이것이 **Transfer Learning**에 대한 아이디어입니다. 예로는 VGG16, inceptionNet, googlenet, Resnet 등이 있습니다.\n",
    "\n",
    "5. 이 커널에서는 크기가 작은 이미지에 대해서 잘 작동하는 pre-trained VGG-16 network를 사용할 것입니다.\n",
    "\n",
    "6.** VGG 아키텍처는 크기가 작은 이미지(CIFAR-10)에서 잘 작동하는 것으로 확인되었습니다. ** 따라서 이 데이터셋에도 잘 작동할 것으로 예상했습니다.\n",
    "\n",
    "1. 코드에는 data augmentation 단계도 포함되어 있으므로 성능이 크게 향상됩니다.\n",
    "\n",
    "2. **GPU가 필요합니다**\n",
    "\n",
    "관심이 있는 분은 여기의 논문 링크를 참고해주세요.\n",
    "[https://arxiv.org/pdf/1409.1556.pdf](http://)\n",
    "\n",
    "keras 라이브러리 설명서입니다.\n",
    "[https://keras.io/applications/#vgg16](http://)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "42618ab1-ea7d-4a8b-98ae-584774306108",
    "_uuid": "c73fefe3108dfd1f26cfda367a8df7283ac4c586"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9d5839b6-f7fb-426d-b05f-cf2dba9313a8",
    "_uuid": "d375d4f754ad7fb77db2142c7c075b4ad4168390"
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../input/train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "test = pd.read_json(\"../input/test.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3cfe5ed2-f8f7-462b-92a9-b2a3a5b40978",
    "_uuid": "88e01459499ec6e3b007b800c9bae0623f3dce7f"
   },
   "source": [
    "\n",
    "\n",
    "Keras 는 pretrained VGG 실행을 제공합니다. 즉, 라이브러리에 있으므로 우리가 만들 필요가 없습니다.\n",
    "여기서 우리는 VGG의 마지막 레이어를 제거하고 바이너리 예측을 위한 시그모이드 레이어를 넣습니다.\n",
    "\n",
    "다음 코드는 모델의 weight가 다운로드 되지 않았기 때문에 kaggle 노트북에서 작동하지 않습니다. 그러나 본인의 노트북에 코드를 복붙하면 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "067f3dd7-3dcf-4b71-857d-e00b4afbd06e",
    "_uuid": "af8be6ce23dba815bbde23fd7e196eb54ae7c4e1"
   },
   "outputs": [],
   "source": [
    "\n",
    "target_train=train['is_iceberg']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce') #inc_angle에서 na 값을 NAN으로 바꿔줌\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#133개의 NA가 있음\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad') #결측값을 앞 방향부터 채워나감\n",
    "X_angle=train['inc_angle']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "X_test_angle=test['inc_angle']\n",
    "\n",
    "# training data를 생성합니다.\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_band_3=(X_band_1+X_band_2)/2 #band_1과 band_2 각 행렬의 값을 더한 평균값으로 만듦. 그래서 band_1과 bnad_2의 크기가 같아야됨.\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "\n",
    "#X_band_1[:, :, :, np.newaxis] -> 같은 배열에 대해 차원을 1차원 증가\n",
    "# axis=-1 설정 -> 맨 마지막 차원 내에서 합쳐짐\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\t\n",
    "\n",
    "#Data Aug for multi-input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size=64\n",
    "# image transformations 정의\n",
    "gen = ImageDataGenerator(horizontal_flip = True, #수평 방향으로 뒤집기\n",
    "                         vertical_flip = True, #수직 방향으로 뒤집기\n",
    "                         width_shift_range = 0., # 지정된 수평방향 이동 범위 내에서 임의로 원본이미지를 이동\n",
    "                         height_shift_range = 0., # 지정된 수직방향 이동 범위 내에서 임의로 원본이미지를 이동\n",
    "                         channel_shift_range=0, # 임의 채널 이동을 위한 범위\n",
    "                         zoom_range = 0.2, # 지정된 확대/축소 범위내에서 임의로 원본이미지를 확대/축소 -> 0.8배에서 1.2배 크기 변화 시킴\n",
    "                         rotation_range = 10) # 지정된 각도 범위내에서 임의로 원본이미지를 회전\n",
    "\n",
    "# 두 generators를 합치는 함수입니다\n",
    "# y와 angle array 모두에 동일한 랜덤 시드의 같은 제네레이터를 사용해야합니다\n",

    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #array는 동일합니다 - 마음은 안정되었으나 training 속도가 느려졌습니다.\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "#  generator를 생성합니다\n",
    "def get_callbacks(filepath, patience=2):\n",
    "   es = EarlyStopping('val_loss', patience=10, mode=\"min\") # 더 이상 개선의 여지가 없으면 학습 조기 종료\n",
    "   msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "   return [es, msave]\n",
    "\n",
    "\n",
    "def getVggAngleModel():\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1, )(input_2) # 출력 뉴런의 수는 1\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, # weights=\"imagenet\" : pre-training on ImageNet\n",
    "                 input_shape=X_train.shape[1:], classes=1) \n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    merge_one = concatenate([x, angle_layer])\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one) # Dropout : 일부 weight만 사용\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2], output=predictions)\n",
    "    \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 데이터 확장과 함께 K-fold Cross Validation을 사용합니다.\n",
    "def myAngleCV(X_train, X_angle, X_test):\n",
    "    K=3\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "\n",
    "        # file path와 callbacks를 정의합니다.\n",
    "        file_path = \"%s_aug_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv, y_train_cv)\n",
    "        galaxyModel= getVggAngleModel()\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=24,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,X_angle_hold], Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        # 최적의 모델을 가져옵니다.\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        # Training score를 가져옵니다.\n",
    "        score = galaxyModel.evaluate([X_train_cv,X_angle_cv], y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        # Test Score를 가져옵니다.\n",
    "        score = galaxyModel.evaluate([X_holdout,X_angle_hold], Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        # validation Score를 가져옵니다.\n",
    "        pred_valid=galaxyModel.predict([X_holdout,X_angle_hold])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        # Test Scores를 가져옵니다.\n",
    "        temp_test=galaxyModel.predict([X_test, X_test_angle])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        # Train Scores를 가져옵니다.\n",
    "        temp_train=galaxyModel.predict([X_train, X_angle])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Log Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    return y_test_pred_log\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea82458f-f41c-4abb-87aa-0dfc7a447969",
    "_uuid": "d462c689ee61d4c1cdcee42c7ded6c7c31c9cddc"
   },
   "outputs": [],
   "source": [
    "preds=myAngleCV(X_train, X_angle, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "012fc91e-17ff-4163-a32d-79007feba4fc",
    "_uuid": "2e7f1db4b36211939fb9650e3b721ac8db09dda2"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=preds\n",
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f2c96b3a-e901-4687-ac8d-7b63ab760bc1",
    "_uuid": "1ec34a0eac4921cb2d6bf5f075367ec6f88005e1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
