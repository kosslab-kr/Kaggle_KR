{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning Model Documentation\n",
    "## describing my solution for the Kaggle competition\n",
    "\n",
    "## “Rossmann Store Sales”\n",
    "\n",
    "### Gert Jacobusse gert.jacobusse@rogatio.nl\n",
    "### Goes, december 2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this competition I had to forecast 6 weeks of store sales for over 1000 Rossmann\n",
    "stores, based on almost three years of sales history. The model I used was extreme\n",
    "gradient boosting (XGBoost [1]) a general purpose tool that is based on decision\n",
    "trees. My feature generation was guided by three main principles: for each train and\n",
    "test record, the model should have features on 1) recent data 2) temporal information\n",
    "and 3) current trends. For feature selection and model ensembling, I heavily exploited\n",
    "a holdout set consisting of the last six weeks of the sales history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection / Extraction\n",
    "For feature extraction, I distinguish three types of features, on 1) recent data 2)\n",
    "temporal information and 3) current trends. I extracted a lot more features then I\n",
    "ended up using.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recent data\n",
    "To create features on recent data, I selected store specific sets of sales data for each\n",
    "month in the the train set (i.e. the three years of sales history). Then for each record, I\n",
    "took the date of that record, and used data from the previous month and further back\n",
    "as the recent history of that record. I extracted features on last quarter, last half year,\n",
    "last year and last 2 years. I also experimented with last month only, but thought it\n",
    "would not be useful to predict sales as far as six weeks ahead.\n",
    "The recent data on store as a whole were still wildly varying, and I identified three\n",
    "features that contributed most to this variance: the day of the week, promotions and\n",
    "holidays. Therefore, I further split out my store specific sets by those three variables\n",
    "and calculated recent averages on diverse combinations of them.\n",
    "To summarize the recent data, I used measures of centrality: median, mean and\n",
    "harmonic mean - and measures of spread: standard deviation, skewness, kurtosis and\n",
    "10%/ 90% percentiles. I also tried to log transform the sales before summarizing, but\n",
    "only one of those transformed variables survived the feature selection.\n",
    "In one variation to the main model, I calculated the recent data features on number of\n",
    "customers, instead of sales amount.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal information\n",
    "For temporal information I created ‘day counters’ to express how each record relates\n",
    "to certain events or cycles. The day counter indicates either the number of days\n",
    "before, after or within the event. As events I had the promotion cycle (every 14 days),\n",
    "the secondary promotion cycle (every three months), the summer holidays (important\n",
    "because they partly took place during the 6 weeks test set), store refurbishments, start\n",
    "of competition and start of secondary promotion cycle. And also the day of week, day\n",
    "of month, and day/ week/ month of year. Apart from the day counters, I also added\n",
    "features on the number of holidays during the current week, last week and next week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current trends\n",
    "To estimate store sales trends, I used data sets about the last quarter and the last year,\n",
    "similar to the data sets for recent data features. Within each dataset, I fit a store\n",
    "specific linear model on 1) the day number - to extrapolate the trend into the six week\n",
    "period 2) day of week and 3) promotions. As a linear model I used Ridge regression\n",
    "from scikits-learn [2] with default regularization parameter, I did not try any\n",
    "alternatives. For each store I also calculated the year over year trend for the previous\n",
    "month, but that feature seems to be of minor importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other information\n",
    "Other important information was about the store: the dataset variables store id,\n",
    "assortment and storetype, together with some aggregates by store: the average sales\n",
    "per customer, the ratio of sales during promotions/ holidays/ Saturdays, the\n",
    "proportion school holidays and the proportion of days that the store is open. Finally,\n",
    "it helped to include data about the state specific weather: maximum temperature an\n",
    "mm precipitation, downloaded from the forum [3].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection of features and Model ensembling\n",
    "I created a lot more features - especially temporal information - than the model could\n",
    "handle. With all features together, the model easily overfit the train set, resulting in\n",
    "suboptimal performance on the test set. Therefore, I needed a way to reduce the\n",
    "feature set and select features that are most helpful to forecast into the test set. As a\n",
    "proxy for the test set, I used my validation set consisting of the last 6 weeks of the\n",
    "train set. I started off by handpicking some combinations that seemed to make sense\n",
    "to me. This way I soon noted that the spread features made overfitting easy. The\n",
    "handpicking cost me a lot of time and I realized that it was biased by my ideas.\n",
    "Therefore I decided to create some models on random selections of the features.\n",
    "Some of the best performing random models gave nice improvements when I\n",
    "ensembled them with my handpicked models. After noting that, I ran over 500\n",
    "random models and systematically calculated the validation error on each pairensemble\n",
    "of models. From the best model pairs I built a larger ensemble, consisting\n",
    "of more than 10 different models (actually the same models with different features).\n",
    "Then I got the idea to take the features from all of the selected models together and\n",
    "combine them into one model: this turned out to work very well - in the end I only\n",
    "kept this combined model together with two of the handpicked models. In this\n",
    "process as a whole, model ensembling and feature selection went hand in hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Techniques and Training\n",
    "For modeling I completely relied on XGBoost and my focus was entirely on feature\n",
    "extraction and selection. I started off from the parameters in the (in)famous public\n",
    "script [4] that performed a lot better on the public leaderboard than I could reproduce\n",
    "within my holdout set. During some experiments, I only changed the number of\n",
    "rounds (from 3000 to 5000) and de column sample by tree (from 0.7 to 0.3 because I\n",
    "had a lot of features).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further enrich and stabilize my ensemble of models, I added separate models that\n",
    "were only trained on the months May to September, over all three years. I chose these\n",
    "months because they cover the test set months and have some 2015 history before the\n",
    "test set. Another ensembling trick was to add ‘month ahead models’ that skipped the\n",
    "most recent month for the calculation of recent/trend features. These ‘month ahead\n",
    "models’ were almost as good as the models that did use all the most recent data. For\n",
    "prediction of the sales in the second month of the test set (where sales in the most\n",
    "recent month were not available at all) I used an ensemble of only ‘month ahead\n",
    "models’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most other teams, I log transformed the dependent variable (sales) and did not\n",
    "include zero sales for training the model. For final ensembling I applied the harmonic\n",
    "mean. I also multiplied all my predictions with a constant factor to improve them. A\n",
    "factor of 0.995 was optimal for both my validation set and the public test set. But I\n",
    "should have applied the mathematical estimate of 0.985 on the forum [5] to achieve a\n",
    "private leaderboard score well below 0.10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "The code is written in Python and consists of a ‘main’ script where the whole\n",
    "procedure takes place by calling functions from three modules: ‘loaddata’ to load the\n",
    "original data into a sqlite database, ‘extractfeatures’ to extract the features and\n",
    "‘runmodels’ to run and ensemble the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code starts with a dictionary ‘features’ that contains all the feature names as\n",
    "keys, and contains a list with four binary entries that indicate in which models each\n",
    "feature is used. After that, two lists are created, defining which models to ensemble\n",
    "for the august and september predictions respectively. After that, all functions from\n",
    "the modules are called in the right order. Information about progress will be printed to\n",
    "stdout by the functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "The following modules are required to run the code.\n",
    "main: os, sqlite3, scipy\n",
    "loaddata: sys, csv\n",
    "extractfeatures: sys, csv, math, numpy, scipy, datetime, pickle, collections, sklearn\n",
    "runmodels: sys, csv, math, numpy, xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How To Generate the Solution (aka README file)\n",
    "The working directory should contain a folder ‘data’, in turn containing ‘train.csv’,\n",
    "‘test.csv’, ‘store.csv’, ‘vakanties.csv’ (for holiday starts, from schulferien.org and\n",
    "mentioned in [6]) and the folder ‘weer’ that contains the weather files. Make sure that\n",
    "dependencies are installed, start running main and have some patience…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Comments and Observations\n",
    "Two findings surprised me: first, that the ‘month ahead models’ performed almost as\n",
    "good as the models that did include information from the most recent month. This\n",
    "implies that recent changes are not very important to predict the sales six weeks\n",
    "ahead. My interpretation of this finding is that the Rossmann sales figures represent a\n",
    "fairly stationary process. Second, I was surprised by the fact that I could use my\n",
    "holdout set to select model pairs from over 500*250 (n*[n-1]/2) pairs without\n",
    "overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all my features made it into the final model selection. Some efforts to capture the\n",
    "annual variation of sales over all stores together, by including monthly or daily sales\n",
    "averages, did not improve the model - I think they were overshadowed by the\n",
    "promotion effects. Also, aggregates by day (what proportion of stores had holidays,\n",
    "what proportion was open - today, yesterday and tomorrow) and aggregates about the\n",
    "spread of sales within each store were not useful. And some additional external data\n",
    "that I tried were not helpful either: state statistics on population and income, monthly\n",
    "sales figures from different sources and information about search term popularity\n",
    "from Google trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Features and Methods\n",
    "I did compare some other models to XGBoost, but none of them were useful, neither\n",
    "on their own nor in the ensemble. Most of the individual XGBoost models already\n",
    "achieve around 0.105 on the private leaderboard - those individual models are simpler\n",
    "in the sense that no meta model ensembling is involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. XGBoost https://github.com/dmlc/xgboost\n",
    "2. Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp.\n",
    "2825-2830, 2011.\n",
    "3. The weather data on the forum: https://www.kaggle.com/c/rossmann-store-sales/\n",
    "forums/t/17058/weather-at-berlin-us-airport\n",
    "4. The (in)famous public script: https://www.kaggle.com/abhilashawasthi/\n",
    "rossmann-store-sales/xgb-rossmann/run/86608\n",
    "5. The forum post about correcting predictions: https://www.kaggle.com/c/\n",
    "rossmann-store-sales/forums/t/17601/correcting-log-sales-prediction-for-rmspe\n",
    "6. The forum thread on external data: https://www.kaggle.com/c/rossmann-storesales/\n",
    "forums/t/17229/external-data-and-other-information?page=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 a/b. Illustration of the task: predict sales six weeks ahead, based on historical\n",
    "sales (only last 3 months of train set shown).\n",
    "\n",
    "Figure 2. This picture illustrates the progress we made in this competition. XGBoost\n",
    "predictions without feature engineering (black) were already quite good. The\n",
    "improvements that full feature engineering (red) gave were really about finetuning.\n",
    "\n",
    "Figure 3. A ‘day counter’ for the time until store refurbishment (last four days on the\n",
    "right of the plot) reveals how the sales are expected to change during the weeks\n",
    "before a refurbishment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
