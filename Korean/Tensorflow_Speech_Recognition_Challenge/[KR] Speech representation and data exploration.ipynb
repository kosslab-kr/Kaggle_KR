{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소개 \n",
    "\n",
    "Hello! This is my very first Kernel. It is meant to give a grasp of a problem of speech representation. I'd also like to take a look on a features specific to this dataset.  \n",
    "이것은 필자의 첫번째 커널이다. 그렇기 때문에 몇가지 문제점이 있을 수 있다. 그래서 필자는 되도록 주어진 데이터셋을 면밀하게 관찰해봤다.\n",
    "\n",
    "Content:<br>\n",
    "내용:  \n",
    "* [1. Visualization of the recordings 음성의 시각화 - input features 입력 데이터 특징 ](#visualization)\n",
    "   * [1.1. Wave and spectrogram 파형과 스펙트로그램 ](#waveandspectrogram)\n",
    "   * [1.2. MFCC](#mfcc)\n",
    "   * [1.3. Sprectrogram in 3d 3차원 스펙트로그램 ](#3d)\n",
    "   * [1.4. Silence removal 조용한 부분 제거](#resampl)\n",
    "   * [1.5. Resampling 리샘플링 - dimensionality reductions 차원축소  ](#silenceremoval)\n",
    "   * [1.6. Features extraction steps 특징 추출 방법 ](#featuresextractionsteps)\n",
    "* [2. Dataset investigation 데이터셋 조사 ](#investigations)\n",
    "   * [2.1. Number of files 파일의 갯수 ](#numberoffiles)\n",
    "   * [2.2. Mean spectrograms and fft 평균 스펙토그램과 FFT ](#meanspectrogramsandfft)\n",
    "   * [2.3. Deeper into recordings 녹음을 더 깊이 살펴보기 ](#deeper)\n",
    "   * [2.4. Length of recordings 녹음의 길이 ](#len)\n",
    "   * [2.5. Note on Gaussian Mixtures modeling 가우시안 복합 모델링 ](#gmms)\n",
    "   * [2.6. Frequency components across the words 주로 반복되는 단어들 ](#components)\n",
    "   * [2.7. Anomaly detection 잡음 탐지  ](#anomaly)\n",
    "* [3. Where to look for the inspiration 영감을 어디에서 얻었나 ](#wheretostart)\n",
    "\n",
    "All we need is here:  \n",
    "우리가 시작하기 전 필요한 것은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Visualization 시각화\n",
    "<a id=\"visualization\"></a> \n",
    "\n",
    "There are two theories of a human hearing - place ( https://en.wikipedia.org/wiki/Place_theory_(hearing) (frequency-based) and temporal (https://en.wikipedia.org/wiki/Temporal_theory_(hearing) )\n",
    "In speech recognition, I see two main tendencies - to input [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) (frequencies), and more sophisticated features MFCC - Mel-Frequency Cepstral Coefficients, PLP. You rarely work with raw, temporal data.\n",
    "\n",
    "음성 인식 이론에는 크게 2가지 이론이 있다. 첫번째는 장소(place)이고, 두번째는  시간(temporal) 이다.  \n",
    "발성 인식에 있어서, 필자는 2가지 경향성을 보임을 확인했다. 첫번째는 스펙트로그램(빈도수)과 두번째는 보다 정교한 특징인 MFCC ( Mel-Frequency Cepstral Coefficients, PLP ) 이다.  독자는 아마도 시간(temporal) 데이터는 거의 사용하지 않을 것이라 생각한다.\n",
    "\n",
    "Let's visualize some recordings!\n",
    "\n",
    "이제 한번 음성 레코드를 시각화 해보자!\n",
    "\n",
    "## 1.1. Wave and spectrogram 음파와 스펙토그램 :\n",
    "<a id=\"waveandspectrogram\"></a> \n",
    "\n",
    "Choose and read some file:  \n",
    "몇가지 파일을 선택하여 읽어보자  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_path = '../input/train/audio/'\n",
    "filename = '/yes/0a7c2a8d_nohash_0.wav'\n",
    "sample_rate, samples = wavfile.read(str(train_audio_path) + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that calculates spectrogram.  \n",
    "스펙토그램을 계산하는 함수를 정의해보자.  \n",
    "\n",
    "Note, that we are taking logarithm of spectrogram values. It will make our plot much more clear, moreover, it is strictly connected to the way people hear.\n",
    "We need to assure that there are no 0 values as input to logarithm.  \n",
    "먼저 우리는 스펙토그램의 로그값들을 가지고 있다는 걸 통해서, 그것이 더욱더 시각화를 명확히 해줌을 기억하자. 그것들은 확실하게 사람들이 듣는 것과 연결되어있기 때문이다.  \n",
    "우리는 입력된 로그값에 0이 아닌 값들이 있어야 하므로 다음과 같은 함수를 정의한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequencies are in range (0, 8000) according to [Nyquist theorem](https://en.wikipedia.org/wiki/Nyquist_rate).  \n",
    "Nyquist의 이론에 따라서, 주파수는 0 ~ 8000 의 값을 가지므로 그걸 시각화 할 수 있다.  \n",
    "\n",
    "Let's plot it:  \n",
    "한번 도표를 만들어보자!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_specgram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b2bad617fbee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_specgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Raw wave of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_specgram' is not defined"
     ]
    }
   ],
   "source": [
    "freqs, times, spectrogram = log_specgram(samples, sample_rate)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Raw wave of ' + filename)\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.imshow(spectrogram.T, aspect='auto', origin='lower', \n",
    "           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "ax2.set_yticks(freqs[::16])\n",
    "ax2.set_xticks(times[::16])\n",
    "ax2.set_title('Spectrogram of ' + filename)\n",
    "ax2.set_ylabel('Freqs in Hz')\n",
    "ax2.set_xlabel('Seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use spectrogram as an input features for NN, we have to remember to normalize features. (We need to normalize over all the dataset, here's example just for one, which doesn't give good *mean* and *std*!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(spectrogram, axis=0)\n",
    "std = np.std(spectrogram, axis=0)\n",
    "spectrogram = (spectrogram - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an interesting fact to point out. We have ~160 features for each frame, frequencies are between 0 and 8000. It means, that one feature corresponds to 50 Hz. However, [frequency resolution of the ear is 3.6 Hz within the octave of 1000 – 2000 Hz](https://en.wikipedia.org/wiki/Psychoacoustics) It means, that people are far more precise and can hear much smaller details than those represented by spectrograms like above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. MFCC\n",
    "<a id=\"mfcc\"></a> \n",
    "\n",
    "If you want to get to know some details about *MFCC* take a look at this great tutorial. [MFCC explained](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/) You can see, that it is well prepared to imitate human hearing properties.\n",
    "\n",
    "You can calculate *Mel power spectrogram* and *MFCC* using for example *librosa* python package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this tutorial\n",
    "# https://github.com/librosa/librosa/blob/master/examples/LibROSA%20demo.ipynb\n",
    "S = librosa.feature.melspectrogram(samples, sr=sample_rate, n_mels=128)\n",
    "\n",
    "# Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
    "log_S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(log_S, sr=sample_rate, x_axis='time', y_axis='mel')\n",
    "plt.title('Mel power spectrogram ')\n",
    "plt.colorbar(format='%+02.0f dB')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n",
    "\n",
    "# Let's pad on the first and second deltas while we're at it\n",
    "delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(delta2_mfcc)\n",
    "plt.ylabel('MFCC coeffs')\n",
    "plt.xlabel('Time')\n",
    "plt.title('MFCC')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classical, but still state-of-the-art systems, *MFCC*  or similar features are taken as the input to the system instead of spectrograms.\n",
    "\n",
    "However, in end-to-end (often neural-network based) systems, the most common input features are probably raw spectrograms, or mel power spectrograms. For example *MFCC* decorrelates features, but NNs deal with correlated features well. Also, if you'll understand mel filters, you may consider their usage sensible.a\n",
    "\n",
    "It is your decision which to choose!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Spectrogram in 3d\n",
    "<a id=\"3d\"></a> \n",
    "\n",
    "By the way, times change, and the tools change. Have you ever seen spectrogram in 3d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Surface(z=spectrogram.T)]\n",
    "layout = go.Layout(\n",
    "    title='Specgtrogram of \"yes\" in 3d',\n",
    "    scene = dict(\n",
    "    yaxis = dict(title='Frequencies', range=freqs),\n",
    "    xaxis = dict(title='Time', range=times),\n",
    "    zaxis = dict(title='Log amplitude'),\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Don't know how to set axis ranges to proper values yet. I'd also like it to be streched like a classic spectrogram above..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Silence removal\n",
    "<a id=\"silenceremoval\"></a> \n",
    "\n",
    "Let's listen to that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(samples, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I consider that some *VAD* (Voice Activity Detection) will be really useful here. Although the words are short, there is a lot of silence in them. A decent *VAD* can reduce training size a lot, accelerating training speed significantly.\n",
    "Let's cut a bit of the file from the beginning and from the end. and listen to it again (based on a plot above, we take from 4000 to 13000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_cut = samples[4000:13000]\n",
    "ipd.Audio(samples_cut, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can agree that the entire word can be heard. It is impossible to cut all the files manually and do this basing on the simple plot. But you can use for example *webrtcvad* package to have a good *VAD*.\n",
    "\n",
    "Let's plot it again, together with guessed alignment of* 'y' 'e' 's'* graphems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs, times, spectrogram_cut = log_specgram(samples_cut, sample_rate)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Raw wave of ' + filename)\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(samples_cut)\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.set_title('Spectrogram of ' + filename)\n",
    "ax2.set_ylabel('Frequencies * 0.1')\n",
    "ax2.set_xlabel('Samples')\n",
    "ax2.imshow(spectrogram_cut.T, aspect='auto', origin='lower', \n",
    "           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "ax2.set_yticks(freqs[::16])\n",
    "ax2.set_xticks(times[::16])\n",
    "ax2.text(0.06, 1000, 'Y', fontsize=18)\n",
    "ax2.text(0.17, 1000, 'E', fontsize=18)\n",
    "ax2.text(0.36, 1000, 'S', fontsize=18)\n",
    "\n",
    "xcoords = [0.025, 0.11, 0.23, 0.49]\n",
    "for xc in xcoords:\n",
    "    ax1.axvline(x=xc*16000, c='r')\n",
    "    ax2.axvline(x=xc, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Resampling - dimensionality reduction\n",
    "<a id=\"resampl\"></a> \n",
    "\n",
    "Another way to reduce the dimensionality of our data is to resample recordings.\n",
    "\n",
    "You can hear that the recording don't sound very natural, because they are sampled with 16k frequency, and we usually hear much more. However, [the most speech related frequencies are presented in smaller band](https://en.wikipedia.org/wiki/Voice_frequency). That's why you can still understand another person talking to the telephone, where GSM signal is sampled to 8000 Hz.\n",
    "\n",
    "Summarizing, we could resample our dataset to 8k. We will discard some information that shouldn't be important, and we'll reduce size of the data.\n",
    "\n",
    "We have to remember that it can be risky, because this is a competition, and sometimes very small difference in performance wins, so we don't want to lost anything. On the other hand, first experiments can be done much faster with smaller training size.\n",
    "\n",
    "We'll need to calculate FFT (Fast Fourier Transform). Definition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])  # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    return xf, vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read some recording, resample it, and listen. We can also compare FFT, Notice, that there is almost no information above 4000 Hz in original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/happy/0b09edd3_nohash_0.wav'\n",
    "new_sample_rate = 8000\n",
    "\n",
    "sample_rate, samples = wavfile.read(str(train_audio_path) + filename)\n",
    "resampled = signal.resample(samples, int(new_sample_rate/sample_rate * samples.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(samples, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(resampled, rate=new_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost no difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf, vals = custom_fft(samples, sample_rate)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('FFT of recording sampled with ' + str(sample_rate) + ' Hz')\n",
    "plt.plot(xf, vals)\n",
    "plt.xlabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf, vals = custom_fft(resampled, new_sample_rate)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('FFT of recording sampled with ' + str(new_sample_rate) + ' Hz')\n",
    "plt.plot(xf, vals)\n",
    "plt.xlabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we reduced dataset size twice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Features extraction steps\n",
    "<a id=\"featuresextractionsteps\"></a> \n",
    "\n",
    "I would propose the feature extraction algorithm like that:\n",
    "1. Resampling\n",
    "2. *VAD*\n",
    "3. Maybe padding with 0 to make signals be equal length\n",
    "4. Log spectrogram (or *MFCC*, or *PLP*)\n",
    "5. Features normalization with *mean* and *std*\n",
    "6. Stacking of a given number of frames to get temporal information\n",
    "\n",
    "It's a pity it can't be done in notebook. It has not much sense to write things from zero, and everything is ready to take, but in packages, that can not be imported in Kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Dataset investigation\n",
    "<a id=\"investigations\"></a> \n",
    "\n",
    "Some usuall investgation of dataset.\n",
    "\n",
    "## 2.1. Number of records\n",
    "<a id=\"numberoffiles\"></a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [f for f in os.listdir(train_audio_path) if isdir(join(train_audio_path, f))]\n",
    "dirs.sort()\n",
    "print('Number of labels: ' + str(len(dirs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate\n",
    "number_of_recordings = []\n",
    "for direct in dirs:\n",
    "    waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n",
    "    number_of_recordings.append(len(waves))\n",
    "\n",
    "# Plot\n",
    "data = [go.Histogram(x=dirs, y=number_of_recordings)]\n",
    "trace = go.Bar(\n",
    "    x=dirs,\n",
    "    y=number_of_recordings,\n",
    "    marker=dict(color = number_of_recordings, colorscale='Viridius', showscale=True\n",
    "    ),\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='Number of recordings in given label',\n",
    "    xaxis = dict(title='Words'),\n",
    "    yaxis = dict(title='Number of recordings')\n",
    ")\n",
    "py.iplot(go.Figure(data=[trace], layout=layout))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is balanced except of background_noise, but that's the different thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Deeper into recordings\n",
    "<a id=\"deeper\"></a> \n",
    "There's a very important fact. Recordings come from very different sources. As far as I can tell, some of them can come from mobile GSM channel.\n",
    "\n",
    "Nevertheless,** it is extremely important to split the dataset in a way that one speaker doesn't occur in both train and test sets.**\n",
    "Just take a look and listen to this two examlpes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['on/004ae714_nohash_0.wav', 'on/0137b3f4_nohash_0.wav']\n",
    "for filename in filenames:\n",
    "    sample_rate, samples = wavfile.read(str(train_audio_path) + filename)\n",
    "    xf, vals = custom_fft(samples, sample_rate)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.title('FFT of speaker ' + filename[4:11])\n",
    "    plt.plot(xf, vals)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Speaker ' + filenames[0][4:11])\n",
    "ipd.Audio(join(train_audio_path, filenames[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Speaker ' + filenames[1][4:11])\n",
    "ipd.Audio(join(train_audio_path, filenames[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also recordings with some weird silence (some compression?):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/yes/01bb6a2a_nohash_1.wav'\n",
    "sample_rate, samples = wavfile.read(str(train_audio_path) + filename)\n",
    "freqs, times, spectrogram = log_specgram(samples, sample_rate)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Spectrogram of ' + filename)\n",
    "plt.ylabel('Freqs')\n",
    "plt.xlabel('Time')\n",
    "plt.imshow(spectrogram.T, aspect='auto', origin='lower', \n",
    "           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "plt.yticks(freqs[::16])\n",
    "plt.xticks(times[::16])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means, that we have to prevent overfitting to the very specific acoustical environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Recordings length\n",
    "<a id=\"len\"></a> \n",
    "\n",
    "Find if all the files have 1 second duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_shorter = 0\n",
    "for direct in dirs:\n",
    "    waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n",
    "    for wav in waves:\n",
    "        sample_rate, samples = wavfile.read(train_audio_path + direct + '/' + wav)\n",
    "        if samples.shape[0] < sample_rate:\n",
    "            num_of_shorter += 1\n",
    "print('Number of recordings shorter than 1 second: ' + str(num_of_shorter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's suprising, and there is a lot of them. We can pad them with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Mean spectrograms and FFT\n",
    "<a id=\"meanspectrogramsandfft\"></a> \n",
    "Let's plot mean FFT for every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = 'yes no up down left right on off stop go'.split()\n",
    "dirs = [d for d in dirs if d in to_keep]\n",
    "\n",
    "print(dirs)\n",
    "\n",
    "for direct in dirs:\n",
    "    vals_all = []\n",
    "    spec_all = []\n",
    "\n",
    "    waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n",
    "    for wav in waves:\n",
    "        sample_rate, samples = wavfile.read(train_audio_path + direct + '/' + wav)\n",
    "        if samples.shape[0] != 16000:\n",
    "            continue\n",
    "        xf, vals = custom_fft(samples, 16000)\n",
    "        vals_all.append(vals)\n",
    "        freqs, times, spec = log_specgram(samples, 16000)\n",
    "        spec_all.append(spec)\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Mean fft of ' + direct)\n",
    "    plt.plot(np.mean(np.array(vals_all), axis=0))\n",
    "    plt.grid()\n",
    "    plt.subplot(122)\n",
    "    plt.title('Mean specgram of ' + direct)\n",
    "    plt.imshow(np.mean(np.array(spec_all), axis=0).T, aspect='auto', origin='lower', \n",
    "               extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "    plt.yticks(freqs[::16])\n",
    "    plt.xticks(times[::16])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Gaussian Mixtures modeling\n",
    "<a id=\"gmms\"></a> \n",
    "\n",
    "We can see that mean FFT looks different for every word. We could model each FFT with a mixture of Gaussian distributions. Some of them however, look almost identical on FFT, like *stop* and *up*... But wait, they are still distinguishable when we look at spectrograms! High frequencies are earlier than low at the beginning of *stop* (probably *s*).\n",
    "\n",
    "That's why temporal component is also necessary. There is a [Kaldi](http://kaldi-asr.org/) library, that can model words (or smaller parts of words) with GMMs and model temporal dependencies with [Hidden Markov Models](https://github.com/danijel3/ASRDemos/blob/master/notebooks/HMM_FST.ipynb).\n",
    "\n",
    "We could use simple GMMs for words to check what can we model and how hard it is to distinguish the words. We can use [Scikit-learn](http://scikit-learn.org/) for that, however it is not straightforward and lasts very long here, so I abandon this idea for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Frequency components across the words\n",
    "<a id=\"components\"></a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violinplot_frequency(dirs, freq_ind):\n",
    "    \"\"\" Plot violinplots for given words (waves in dirs) and frequency freq_ind\n",
    "    from all frequencies freqs.\"\"\"\n",
    "\n",
    "    spec_all = []  # Contain spectrograms\n",
    "    ind = 0\n",
    "    for direct in dirs:\n",
    "        spec_all.append([])\n",
    "\n",
    "        waves = [f for f in os.listdir(join(train_audio_path, direct)) if\n",
    "                 f.endswith('.wav')]\n",
    "        for wav in waves[:100]:\n",
    "            sample_rate, samples = wavfile.read(\n",
    "                train_audio_path + direct + '/' + wav)\n",
    "            freqs, times, spec = log_specgram(samples, sample_rate)\n",
    "            spec_all[ind].extend(spec[:, freq_ind])\n",
    "        ind += 1\n",
    "\n",
    "    # Different lengths = different num of frames. Make number equal\n",
    "    minimum = min([len(spec) for spec in spec_all])\n",
    "    spec_all = np.array([spec[:minimum] for spec in spec_all])\n",
    "\n",
    "    plt.figure(figsize=(13,7))\n",
    "    plt.title('Frequency ' + str(freqs[freq_ind]) + ' Hz')\n",
    "    plt.ylabel('Amount of frequency in a word')\n",
    "    plt.xlabel('Words')\n",
    "    sns.violinplot(data=pd.DataFrame(spec_all.T, columns=dirs))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violinplot_frequency(dirs, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violinplot_frequency(dirs, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violinplot_frequency(dirs, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Anomaly detection\n",
    "<a id=\"anomaly\"></a> \n",
    "\n",
    "We should check if there are any recordings that somehow stand out from the rest. We can lower the dimensionality of the dataset and interactively check for any anomaly.\n",
    "We'll use PCA for dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_all = []\n",
    "names = []\n",
    "for direct in dirs:\n",
    "    waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n",
    "    for wav in waves:\n",
    "        sample_rate, samples = wavfile.read(train_audio_path + direct + '/' + wav)\n",
    "        if samples.shape[0] != sample_rate:\n",
    "            samples = np.append(samples, np.zeros((sample_rate - samples.shape[0], )))\n",
    "        x, val = custom_fft(samples, sample_rate)\n",
    "        fft_all.append(val)\n",
    "        names.append(direct + '/' + wav)\n",
    "\n",
    "fft_all = np.array(fft_all)\n",
    "\n",
    "# Normalization\n",
    "fft_all = (fft_all - np.mean(fft_all, axis=0)) / np.std(fft_all, axis=0)\n",
    "\n",
    "# Dim reduction\n",
    "pca = PCA(n_components=3)\n",
    "fft_all = pca.fit_transform(fft_all)\n",
    "\n",
    "def interactive_3d_plot(data, names):\n",
    "    scatt = go.Scatter3d(x=data[:, 0], y=data[:, 1], z=data[:, 2], mode='markers', text=names)\n",
    "    data = go.Data([scatt])\n",
    "    layout = go.Layout(title=\"Anomaly detection\")\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(figure)\n",
    "    \n",
    "interactive_3d_plot(fft_all, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are *yes/e4b02540_nohash_0.wav*, *go/0487ba9b_nohash_0.wav* and more points, that lie far away from the rest. Let's listen to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recording go/0487ba9b_nohash_0.wav')\n",
    "ipd.Audio(join(train_audio_path, 'go/0487ba9b_nohash_0.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recording yes/e4b02540_nohash_0.wav')\n",
    "ipd.Audio(join(train_audio_path, 'yes/e4b02540_nohash_0.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you will look for anomalies for individual words, you can find for example this file for *seven*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording seven/e4b02540_nohash_0.wav\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ipd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-affcbbe569fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recording seven/e4b02540_nohash_0.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mipd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_audio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seven/b1114e4f_nohash_0.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ipd' is not defined"
     ]
    }
   ],
   "source": [
    "print('Recording seven/e4b02540_nohash_0.wav')\n",
    "ipd.Audio(join(train_audio_path, 'seven/b1114e4f_nohash_0.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's nothing obviously important. Usually you can find some distortions using this method. Data seems to contain what it should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Where to look for the inspiration\n",
    "<a id=\"wheretostart\"></a> \n",
    "\n",
    "You can take many different approches for the competition. I can't really advice any of that. I'd like to share my initial thoughts.\n",
    "\n",
    "There is a trend in recent years to propose solutions based on neural networks. Usually there are two architectures. My ideas are here.\n",
    "\n",
    "1. Encoder-decoder: https://arxiv.org/abs/1508.01211\n",
    "2. RNNs with CTC loss: https://arxiv.org/abs/1412.5567<br>\n",
    "For me, 1 and 2  are a sensible choice for this competition, especially if you do not have background in SR field. They try to be end-to-end solutions. Speech recognition is a really big topic and it would be hard to get to know important things in short time.\n",
    "\n",
    "3. Classic speech recognition is described here: http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf\n",
    "\n",
    "You can find *Kaldi* [Tutorial for dummies](http://kaldi-asr.org/doc/kaldi_for_dummies.html), with a problem similar to this competition in some way.\n",
    "\n",
    "4. Very deep CNN - Don't know if it is used for SR. However, most papers concern Large Vocabulary Continuous Speech Recognition Systems (LVCSR). We got different task here - a very small vocabulary, and recordings with only one word in it, with a (mostly) given length. I suppose such approach can win the competition. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**If you like my work please upvote.**\n",
    "\n",
    "Leave a feedback that will let me improve! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
